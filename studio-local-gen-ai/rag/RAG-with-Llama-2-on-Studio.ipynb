{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8104cd3e-68df-457a-bcc6-f42c3133d861",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Question Answering with Llama 2, LangChain and Pinecone using SageMaker Studio Notebooks for fast experimentation\n",
    "\n",
    "In this notebook, we demonstrate the use of Llama2 text generation combined with the HuggingFace Embedding model to efficiently construct a Retrieval Augmented Generation (RAG) QnA system on Studio Notebooks. This notebook, powered by Pytorch 2.0.0 Image and an ml.g5.2xlarge instance, enables the download of open-source HuggingFace models. These are converted into local LLMs, which we then use to build, experiment with, tune, and deploy the LLM for a RAG application framework. Additionally, we showcase how the PineCone Embedding store can be utilized to archive and retrieve embeddings, integrating it into your RAG workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797e87af-8a8e-4bae-836b-7398a281802f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"background-color: #FFDDDD; border-left: 5px solid red; padding: 10px; color: black;\">\n",
    "    <strong>Kernel:</strong> PyTorch 2.0.0 Python 3.10 GPU Optimized <strong>Instance Type:</strong> ml.g5.2xlarge\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdbbf41-b8ff-4395-bb3a-80717ffc5303",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 01. Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3664a39-3158-447a-88b2-cb5e9a81a8fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Install the required libriaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d771379-e7b4-4fe3-a2b0-64e6c783c3fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "sagemaker>=2.175.0\n",
    "transformers==4.33.0\n",
    "accelerate==0.21.0\n",
    "datasets==2.13.0\n",
    "langchain==0.0.297\n",
    "pypdf>=3.16.3\n",
    "pinecone-client\n",
    "sentence_transformers\n",
    "safetensors>=0.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06a51b5-f307-48a7-aa0b-5ba2fac44f4f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e1778-d2e5-41a4-b6db-c377cdbafba8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 02. Load Llama-2 7B chat in the notebook for experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fa4d6d-7b9b-4b35-a270-c3fed44ce5e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, let's download the Llama-2-7b-chat-hf model from the Hugging Face Hub. Llama 2 models are gated, to get access follow the instructions [here](https://huggingface.co/meta-llama/Llama-2-7b-hf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33848c91-23a5-4af9-bdb1-c4a98eaa8a88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "hf_access_token = getpass.getpass(\"Huggingface API Token:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b0694-aacd-4d7f-9792-ba416dcfdf24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    LlamaTokenizer, \n",
    "    LlamaForCausalLM, \n",
    "    GenerationConfig,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc6f9c2-bd28-40ce-89a4-ccc09ea3d941",
   "metadata": {},
   "source": [
    "The following cell takes few minutes to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f69641-86ac-4532-92d1-19b7ce4b46a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tg_model_id = \"meta-llama/Llama-2-7b-chat-hf\" #the model id in Hugging Face\n",
    "tg_model_path = f\"./tg_model/{tg_model_id}\" #the local directory where the model will be saved\n",
    "\n",
    "if  not (os.path.exists(tg_model_path)) or os.listdir(tg_model_path and tg_model_path) == []:\n",
    "    print(\"Loading model from HuggingFace\")\n",
    "\n",
    "    tg_model = AutoModelForCausalLM.from_pretrained(\n",
    "        tg_model_id, \n",
    "        token=hf_access_token,\n",
    "        do_sample=True, \n",
    "        use_safetensors=True,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    tg_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tg_model_id, \n",
    "        token=hf_access_token\n",
    "    )\n",
    "\n",
    "    tg_model.save_pretrained(\n",
    "        save_directory=tg_model_path, \n",
    "        from_pt=True\n",
    "    )\n",
    "    tg_tokenizer.save_pretrained(\n",
    "        save_directory=tg_model_path, \n",
    "        from_pt=True\n",
    "    )\n",
    "else:\n",
    "    print(\"Loading from model from local directory\")\n",
    "    tg_model = LlamaForCausalLM.from_pretrained(\n",
    "       tg_model_path,\n",
    "       device_map=\"auto\"\n",
    "    )\n",
    "    tg_tokenizer = AutoTokenizer.from_pretrained(tg_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d7126-5d9c-4cc5-abd8-b0e3551dcc12",
   "metadata": {},
   "source": [
    "Check memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9521f112-fb50-4bb7-a0d9-d1c481429665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Memory allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"Memory reserved  %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"Max memory reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53d8188-0bf0-44f4-93d9-b2e4ead0bc17",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 03. Simple question-answering using Llama 2 7B chat and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5875641-131d-4ebf-bcef-a50c2834b845",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now that the model is available in memory, we can start using it to answer questions. The Llama-2 chat models expect the prompt to follow the below format:\n",
    "\n",
    "    \n",
    "\\<s>[INST] <\\<SYS\\>>\n",
    "\n",
    "{{ system_prompt }}\n",
    "\n",
    "\\<<SYS\\>>\n",
    "\n",
    "{{ user_message }} [/INST]\n",
    "\n",
    "   \n",
    "where\n",
    "- \\<s> - is the beginning of the sequence.\n",
    "- <\\<SYS>> - is the beginning of the system message.\n",
    "- \\<</SYS\\>> - is the end of the system message.\n",
    "- [INST] - is the beginning of the instructions\n",
    "- [/INST] - is the end of the instructions\n",
    "\n",
    "Let's create a recipe based on the above that will helps us define our prompts going forward. For that we will use [PromptTemplate](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/) from LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4d5e11-2a1c-4981-a16d-a305325d0c34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"<s>[INST] <<SYS>>\\nYou are an assistant for question-answering tasks. You are helpful and friendly. Use the following pieces of retrieved context to answer the query. If you don't know the answer, you just say I don't know. Use three sentences maximum and keep the answer concise.\n",
    "<<SYS>>\\n\n",
    "{context}\\n\n",
    "{question} [/INST]\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(\n",
    "    template=template, \n",
    "    input_variables=['context','question']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d6a863-6d09-43a3-a21d-ec52de136bf6",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we test the model on some questions without providing any context. For our tests, we will use questions about AWS news from 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c9fb93-b816-40a1-a97b-5726a96f400b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question= \"When can I visit the AWS M&E Customer Experience Center in New York City?\"\n",
    "question2 = \"How many awards have AWS Media Services won in 2023?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7500142a-8cc4-45dd-a0f0-e46abb7d0b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tg_tokenizer.add_special_tokens(\n",
    "    {\"pad_token\": \"[PAD]\"}\n",
    ")\n",
    "tg_tokenizer.padding_side = \"left\"\n",
    "\n",
    "tg_pipe = transformers.pipeline(\n",
    "    task='text-generation',\n",
    "    model=tg_model, \n",
    "    tokenizer=tg_tokenizer,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tg_tokenizer.eos_token_id,\n",
    "    pad_token_id=tg_tokenizer.eos_token_id,\n",
    "    max_new_tokens=300,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df8b60c-a237-46ad-9061-36bab130a263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm=HuggingFacePipeline(pipeline=tg_pipe, model_kwargs={'temperature':0.7})\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "no_context_response = llm_chain.predict(context=\"\", question=question)\n",
    "print(no_context_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074dabc6-d77e-4a98-85dc-12514b6b0783",
   "metadata": {},
   "source": [
    "Let's see if we can improve this answer by adding information from the AWS blog post [AWS announces new M&E Customer Experience Center in New York City](https://aws.amazon.com/blogs/media/aws-announces-new-me-customer-experience-center-in-new-york-city/) to our prompt as context to see if that improves the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39727de5-6eef-492c-969c-013a8fe64e2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = \"\"\"Media and entertainment (M&E) customers continue to face challenges in creating more content, \n",
    "more quickly, and distributing it to more endpoints than ever before in their quest to delight viewers globally. \n",
    "Amazon Web Services (AWS), along with AWS Partners, have showcased the rapid evolution of M&E solutions for years at industry events \n",
    "like the National Association of Broadcasters (NAB) Show and the International Broadcast Convention (IBC). Until now, AWS for M&E technology demonstrations\n",
    "were accessible in this way just a few weeks out of the year. Customers are more engaged than ever before; they want to have higher quality conversations \n",
    "regarding user experience and media tooling. These conversations are best supported by having an interconnected solution architecture for reference.\n",
    "Scheduling a visit of the M&E Customer Experience Center will be available starting November 13th, please send an email to AWS-MediaEnt-CXC@amazon.com..\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d4814-21bc-4448-a08a-0eb87025c219",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context_response = llm_chain.predict(context=context, question=question)\n",
    "print(context_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c252fb6a-305b-43fc-9de5-dfba1b6387cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 04. RAG question answering with Llama 2 7B chat, LangChain and Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c762de8c-0a83-4d64-a4cd-f45f7684da69",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "In the above response, the model provides an answer with data from 2023 based on the context we provided. Next we want to scale this approach using __Retrieval Augmented Generation (RAG)__.\n",
    "With RAG, we will ingest external data into our knowledge base and augment the prompt by adding only the data that is relevant to the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc01627-1b6d-4366-9fc9-ab87661f3700",
   "metadata": {
    "tags": []
   },
   "source": [
    "For our example, we will use 2 AWS blog posts as external files. These are already available as PDF files in the data folder of this project.\n",
    "1. [AWS Media Services awarded industry accolades](https://aws.amazon.com/blogs/media/aws-media-services-awarded-industry-accolades/)\n",
    "2. [AWS announces new M&E Customer Experience Center in New York City](https://aws.amazon.com/blogs/media/aws-announces-new-me-customer-experience-center-in-new-york-city/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1b4d5b-0cfb-422b-aa90-ed481b48194d",
   "metadata": {},
   "source": [
    "After that, we split files into documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc83614a-a646-4d4b-ae6f-8d5f6964c038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"./data/\")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700,\n",
    "    chunk_overlap=5,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702ef35-4338-4aa4-9a4a-481a5db011dc",
   "metadata": {},
   "source": [
    "Next, we generate the embeddings for the documents. For that we will use the [bge-small-en](https://huggingface.co/BAAI/bge-small-en) model. We use HuggingFace transfomers to download it to the local directory and load it in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb59413-1ae8-4213-bd96-434bfc218674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "em_model_name = \"BAAI/bge-small-en\"\n",
    "em_model_path = f\"./em-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c45182e-e564-438b-b3d4-2741b466a83d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "em_model = AutoModel.from_pretrained(\n",
    "    em_model_name,\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "em_tokenizer = AutoTokenizer.from_pretrained(em_model_name, device_map=\"cuda\")\n",
    "\n",
    "# save model to disk\n",
    "em_tokenizer.save_pretrained(\n",
    "    save_directory=f\"{em_model_path}/model\", \n",
    "    from_pt=True\n",
    ")\n",
    "\n",
    "em_model.save_pretrained(\n",
    "    save_directory=f\"{em_model_path}/model\", \n",
    "    from_pt=True\n",
    ")\n",
    "em_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e884ee-5762-4674-a4fe-8dec36b9cfa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize sentences\n",
    "def tokenize_text(_input, device):\n",
    "    return em_tokenizer(\n",
    "        [_input], \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "\n",
    "# Run embedding task a function with model and text sentences as input\n",
    "def embedding_generator(_input, normalize=True):\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        embedded_output = em_model(\n",
    "            **tokenize_text(\n",
    "                _input, \n",
    "                em_model.device\n",
    "            )\n",
    "        )\n",
    "        sentence_embeddings = embedded_output[0][:, 0]\n",
    "        # normalize embeddings\n",
    "        if normalize:\n",
    "            sentence_embeddings = torch.nn.functional.normalize(\n",
    "                sentence_embeddings, \n",
    "                p=2, \n",
    "                dim=1\n",
    "            )\n",
    "    \n",
    "    return sentence_embeddings[0, :].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1944501e-6b6b-4a03-bf35-db5b59502d20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_sentence_embedding = embedding_generator(docs[0].page_content)\n",
    "print(f\"Embedding size of the document --->\", len(sample_sentence_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff3bd75-c8d8-4c25-afe0-25406249be72",
   "metadata": {},
   "source": [
    "We are now ready to ingest the embeddings into our vector store. In this notebook we will use [Pinecone](https://www.pinecone.io/), however you can replace the below code with that for the vector store of your choice.\n",
    "If you don't have a Pinecone account you can sign up for free to complete this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdf57ca-368e-4275-9a7b-f23d6831abb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#enter your Pinecone keys\n",
    "os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\n",
    "os.environ[\"PINECONE_ENV\"] = getpass.getpass(\"Pinecone Environment:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3167c11c-3900-4fb2-88e9-27dabe57186d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#initialize Pinecone\n",
    "import pinecone\n",
    "pinecone.init(\n",
    "    api_key = os.environ[\"PINECONE_API_KEY\"],\n",
    "    environment = os.environ[\"PINECONE_ENV\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ead9204-a1c6-4e9c-9b83-6608a00e7a54",
   "metadata": {},
   "source": [
    "In Pinecone, we create a new vector search index and ingest the embeddings we created in the previous step. The size of the index is the dimension of our embeddings model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ffc6e6-f9b6-45f4-a87c-473f24098368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#check if index already exists, if not we create it\n",
    "index_name = \"rag-index\"\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        dimension=len(sample_sentence_embedding),\n",
    "        metric='cosine'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7089eae7-f173-476e-9004-535893cfd2ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#insert the embeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "vector_store = Pinecone.from_documents(\n",
    "    docs, \n",
    "    embedding_generator, \n",
    "    index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439c8ed5-dcb2-45ab-9737-c75043848e0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's do a quick test to see if the similarity search is working well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8f1253-5cb9-4225-a7fe-31e28fde496f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = vector_store.similarity_search(question)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23974b7-71dc-4024-9b60-dcbe42dace73",
   "metadata": {
    "tags": []
   },
   "source": [
    "We have Llama-2 chat model in memory and the embeddings inserted in our Pinecone index. To improve the responses of the Llama 2 chat model we bring it alltogether and implement the RAG architecture easily with the Langchain [RetrievalQA](https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa). RetrievalQA augments our initial prompt with the most similar documents from the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d4c5f9-5c2d-4272-83b7-f106486da0d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm_qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe4e48a-e4e2-4d92-a349-f1df63121b20",
   "metadata": {},
   "source": [
    "And that's it! Let's ask the model again to see if we will get 2023 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d80570-d172-42d4-aa89-e3eceb115111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "#helper method to improve the readability of the response\n",
    "def print_response(llm_response):\n",
    "    temp = [textwrap.fill(line, width=100) for line in llm_response['result'].split('\\n')]\n",
    "    response = '\\n'.join(temp)\n",
    "    print(f\"{llm_response['query']}\\n \\n{response}'\\n \\n Source Documents:\")\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c7fdf0-8665-431b-a2a9-c6ca97836fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_response(llm_qa_chain(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bb6772-54bb-40e4-bd67-fbdc6c896644",
   "metadata": {
    "tags": []
   },
   "source": [
    "The model returns a more informed response with details from 2023 and the pages in the documents from where it acquired the information. \n",
    "\n",
    "Let's try another question. The answer to this one is in a different document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00db9ed9-4124-41ad-8750-69005b2fbd94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_response(llm_qa_chain(question2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c6aafb-8dee-4dfc-b50e-0a4c3a384f7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can continue our experimentation with more files, different model parameters and different questions. Once we have sufficiet confidence in our approach, \n",
    "we can deploy our models to Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d13e2e9-4257-4312-9fe2-e4dd581cd238",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 04. Supercharge your applications with GenAI by deploying your models to Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6df67-2b16-454e-af38-885eac2f5cab",
   "metadata": {
    "tags": []
   },
   "source": [
    "First we import the required libraries, and retrieve the IAM role and session we will use for deployment.  To deploy a model to a SageMaker endpoint, we first need to compress the model artifacts and upload the tar.gz file to Amazon S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c78cd-d3d3-43f8-9264-4ce4a97d21b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 04a. Deploy Text Generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa5920f-fbc1-4fae-9952-df84ef211acb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = sagemaker.Session().boto_region_name\n",
    "bucket = sess.default_bucket() # Set a default S3 bucket\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "prefix = 'qa-rag-models-test/rag-blog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f540b9f6-4422-46d8-877b-9effd162b53c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrained_model_location = f\"s3://{bucket}/{prefix}/llama-2-7B-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea17fdee-269e-47f6-acb9-f7972258a5e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_path = sagemaker.s3.S3Uploader.upload(tg_model_path, pretrained_model_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186dcc74-8b08-4407-9293-c986178c58f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "djl_properties_filename = \"serving.properties\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450f48fa-80f7-472b-b3fa-2fed59e7600b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {djl_properties_filename}\n",
    "engine = MPI\n",
    "option.tensor_parallel_degree = 1\n",
    "option.rolling_batch = auto\n",
    "option.max_rolling_batch_size = 64\n",
    "option.model_loading_timeout = 3600\n",
    "option.paged_attention = true\n",
    "option.trust_remote_code = true\n",
    "option.dtype = fp16\n",
    "option.rolling_batch=lmi-dist\n",
    "option.max_rolling_batch_prefill_tokens=1560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbac860-dba1-428b-a1d8-78d5476b86b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!echo -n \"option.s3url = $pretrained_model_location\" >> {djl_properties_filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7856a2f-ed96-46b8-ac22-2bdad4930739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelfile_base_name = f\"local-{tg_model_id.replace('/', '-')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5a6648-204c-4ddc-9699-6f453e40f8bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir {modelfile_base_name}\n",
    "!mv serving.properties {modelfile_base_name}/\n",
    "!tar czvf {modelfile_base_name}.tar.gz {modelfile_base_name}/\n",
    "!rm -rf {modelfile_base_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e8fef6-ff1c-4318-bea8-8aab69d07cad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list out the contents of the tar gz file for validation\n",
    "!tar -ztvf {modelfile_base_name}.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec63ea6f-83dc-4e25-931f-a44af164e8bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\", region=region, version=\"0.23.0\"\n",
    ")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75196f0-46b5-47a8-b246-f1fe56233c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload file and instantiate a new SageMaker Model\n",
    "s3_code_prefix = \"large-model-lmi/artifacts\"\n",
    "\n",
    "code_artifact = sess.upload_data(f\"{modelfile_base_name}.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531a0809-b888-40dc-b053-23b20090b954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama2_model_name = sagemaker.utils.name_from_base(\n",
    "    f\"{tg_model_id.replace('/', '-')}\"\n",
    ")\n",
    "\n",
    "tg_sm_model = Model(\n",
    "    sagemaker_session=sess,\n",
    "    image_uri=inference_image_uri,\n",
    "    model_data=code_artifact,\n",
    "    role=role,\n",
    "    name=llama2_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce3e4f-b0fe-44d9-afe8-477ad042a5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.2xlarge\"\n",
    "endpoint_name = f\"ep-{llama2_model_name}\"\n",
    "\n",
    "tg_sm_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    container_startup_health_check_timeout=900,\n",
    "    wait=False, # <-- Set to True, if you would prefer to wait for the endpoint to spin up\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a750c8-9150-4040-8b59-4763d1827dd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Endpoint name to use ---> {tg_sm_model.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad5f953-274e-4ede-be23-2e3f0e3caea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=tg_sm_model.endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4796940c-6fa5-4114-abd8-36009fa2a370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.predict(\n",
    "    {\n",
    "        \"inputs\": \"Who is the president of Brazil?\",\n",
    "        \"parameters\": {\"temperature\": 0.1, \"max_new_tokens\": 50}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8cfb35-850a-44f6-8007-d3cc379a9ff2",
   "metadata": {},
   "source": [
    "### 04b. Deploy Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d2f91-8530-4539-82f6-6ed6c506246d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {em_model_path}/model.py\n",
    "from djl_python import Input, Output\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModel, \n",
    "    AutoTokenizer\n",
    ")\n",
    "from typing import Any, Dict, Tuple\n",
    "import deepspeed\n",
    "import warnings\n",
    "import tarfile\n",
    "\n",
    "model, tokenizer = None, None\n",
    "model_dir = \"./model/\"\n",
    "\n",
    "\n",
    "def get_model(properties):\n",
    "    \n",
    "    local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "    \n",
    "    print(f\"Loading model from {model_dir}\")\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_dir\n",
    "    )\n",
    "    \n",
    "    model = deepspeed.init_inference(\n",
    "        model,\n",
    "        mp_size=properties[\"tensor_parallel_degree\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"Loading tokenizer from {model_dir}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def handle(inputs: Input) -> None:\n",
    "    global model, tokenizer\n",
    "    \n",
    "    if not model:\n",
    "        model, tokenizer = get_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        return None\n",
    "    \n",
    "    data = inputs.get_as_json()\n",
    "    text = data[\"text\"]\n",
    "    \n",
    "    input_tokenized = tokenizer(\n",
    "        [text], \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors='pt'\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    outputs = model(**input_tokenized)\n",
    "    \n",
    "    sentence_embeddings = outputs[0][:, 0]\n",
    "    \n",
    "    # normalize embeddings\n",
    "    sentence_embeddings = torch.nn.functional.normalize(\n",
    "        sentence_embeddings, \n",
    "        p=2, \n",
    "        dim=1\n",
    "    )\n",
    "    sentence_embeddings = sentence_embeddings[0, :].tolist()\n",
    "    \n",
    "    result = {\"outputs\": sentence_embeddings}\n",
    "    \n",
    "    return Output().add(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2662467-c8a3-42c9-afc4-7b30d983f86c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {em_model_path}/requirements.txt\n",
    "einops\n",
    "git+https://github.com/lanking520/DeepSpeed.git@falcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf2d608-f949-4f7b-a8d4-e4e14dae864e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {em_model_path}/serving.properties\n",
    "engine=DeepSpeed\n",
    "option.tensor_parallel_degree=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc3ba20-cc94-4e8b-bb70-cc5482cd73a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm embeddings-model.tar.gz\n",
    "!rm -rf {em_model_path}/.ipynb_checkpoints\n",
    "!cd {em_model_path} && tar -czvf ../embeddings-model.tar.gz ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56b620f-64f2-47ef-8d81-1776079066f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar -tzvf embeddings-model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9231f7-cfd0-4cdc-b003-b64b0f6d1f96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_inference_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\", region=region, version=\"0.23.0\"\n",
    ")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f406a-77b2-4917-bd13-f2deca1b4e38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedded_code_artifact = sess.upload_data(\"embeddings-model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {embedded_code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398dd9dc-dffc-4ddf-8b6b-3347c46b07e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model_name = sagemaker.utils.name_from_base(\n",
    "    f\"{em_model_name.replace('/', '-')}\"\n",
    ")\n",
    "\n",
    "em_sm_model = Model(\n",
    "    sagemaker_session=sess,\n",
    "    image_uri=embedding_inference_image_uri,\n",
    "    model_data=embedded_code_artifact,\n",
    "    role=role,\n",
    "    name=embedding_model_name,\n",
    ")\n",
    "print(f\"Creating a new model ---> {em_sm_model.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dd6a4a-4014-40c1-937c-436a57ee92fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_instance_type = \"ml.g5.2xlarge\"\n",
    "\n",
    "em_sm_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=embedding_instance_type,\n",
    "    endpoint_name=f\"ep-{embedding_model_name}\",\n",
    "    container_startup_health_check_timeout=900,\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9f81c1-be49-4789-8dd7-92e1590cd818",
   "metadata": {},
   "source": [
    "## 05. Run LangChain Inference using SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faab706-c63b-49fe-a3ac-e0718aab3e7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.llms import SagemakerEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ba435-b220-4ce4-a662-f5d03d32c235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        body = {\n",
    "            \"inputs\": prompt, \n",
    "            \"parameters\": model_kwargs\n",
    "        }\n",
    "        input_str = json.dumps(body)\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"generated_text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d91047-1aad-4c07-8f84-afe677b3e3e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "content_handler = ContentHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9aeb46-aeb5-427d-895c-25271b82822e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert your local LLM into SageMaker endpoint LLM\n",
    "llm_sm_ep = SagemakerEndpoint(\n",
    "    endpoint_name=tg_sm_model.endpoint_name, # <--- Your endpoint name\n",
    "    region_name=region,\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.05, \n",
    "        \"max_new_tokens\": 512\n",
    "    },\n",
    "    content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6d250c-c9cf-4d43-8f8b-075b13cb3d02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_qa_smep_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm_sm_ep,\n",
    "    chain_type='stuff',\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1390fab0-e427-4264-8cd5-5711f8f762ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_response(llm_qa_smep_chain(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be62da26-9769-48ff-81b3-de5bb1d01703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_response(llm_qa_smep_chain(question2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4510777a-8caf-4391-871f-cc3ec1041dab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 05a. Invoke the Embedding Endpoint for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebd5bb5-5f12-4025-97da-950a6c7222fe",
   "metadata": {},
   "source": [
    "This section shows you how to invoke your custom embedding endpoint for inference.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5daf68-2b58-4ad8-9741-dc1b9abb7565",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=em_sm_model.endpoint_name,\n",
    "    Body=json.dumps({\n",
    "        \"text\": \"This is a sample text\"\n",
    "    }),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "outputs = json.loads(response_model[\"Body\"].read().decode(\"utf8\"))['outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c9e4a9-efb8-40c5-b7d1-bd9e52f914a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Sample embeddings ---> {outputs[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fb1c0f-0541-4ae0-9c69-83b0e838d47f",
   "metadata": {},
   "source": [
    "## 06. Clean Up Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c5cc5-1b43-41c1-934c-579eca55829d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete your text generation endpoint\n",
    "sm_client.delete_endpoint(\n",
    "    EndpointName=tg_sm_model.endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0063170-9664-4043-b0bd-57919088da48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete your text embedding endpoint\n",
    "sm_client.delete_endpoint(\n",
    "    EndpointName=em_sm_model.endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4521826-004f-4727-a4c7-411bf0fed738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
